---
title: "<center>IDS-Final Project</center>"
author: "<center>Avisek Choudhury, Aldo Adriazola, Kait Arnold</center>"
date: "<center>3/08/2020</center>"
output:
  pdf_document: 
    toc: yes
  html_document: 
    toc: yes
---

## Dataset

The physicians have identified a data set that consists of over 500 measurements from Fine Needle Aspiration (FNA) of breast tissue masses. In an FNA, a small needle is used to extract a sample of cells from a tissue mass. The cells are then photographed under a microscope. The resulting photographs are entered into graphical imaging software. A trained technician uses a mouse pointer to draw the boundary of the nuclei. The software then calculates each of ten characteristics for the nuclei. This process is repeated for most or all of the nuclei in the sample.

The data consists of measurements of the cell nuclei for the following characteristics: 

1. radius 
2. texture 
3. perimete r
4. area 
5. smoothness (local variation in radius lengths) 
6. compactness (perimeter^2 / area - 1.0) 
7. concavity (severity of concave portions of the contour) 
8. concave points (number of concave portions of the contour) 
9. symmetry 
10. fractal dimension ("coastline approximation" - 1) 

Measurements of these ten characteristics are summarized for all cells in the sample. The dataset consists of the mean, standard error of the mean, and maximum of the 10 characteristics, for a total of 30 observations for each. Additionally, the data set includes an identification number and a variable that indicates if the tissue mass is malignant (M) or benign (B).

```{r readCSV, warning=FALSE, error=TRUE, message=FALSE}
#Load the necessary libraries
library(tidyverse)
library(class)
library(caret)
library(rpart)
library(partykit)
library(randomForest)
```


# 1. Download the data from NeXus: FNA_cancer.csv

```{r readCSV, warning=FALSE, error=TRUE, message=FALSE}
#Load the dataset that was previously downloaded
cancer_df <- read_csv('C:/MSDS/Spring 2020/IDS/Project/FNA_cancer.csv')

#Print the dataset
head(cancer_df)
```


# 2. Perform basic exploratory data analysis.

## Exploratory data analysis (EDA)

```{r fixColumn, warning=FALSE, error=TRUE, message=FALSE}
#Make the Diagnosis as Factor
cancer_df$diagnosis <- as.factor(cancer_df$diagnosis)

#Fix the column names for consistency
colnames(cancer_df) <- str_replace(colnames(cancer_df), ' ', '_')
```


```{r combHistMean, warning=FALSE, error=TRUE, message=FALSE}
# ggplot(cancer_df[ , c(3:12)] %>% gather(), aes(value)) + 
#     geom_histogram(bins = 10) + 
#     facet_wrap(~key, scales = 'free_x')
# create a histogram for the mean measurements
ggplot(cancer_df[ , c(2:12)] %>% 
         pivot_longer(cols = radius_mean:fractal_dimension_mean), 
       aes(value, fill = diagnosis)) +
    geom_histogram(bins = 10) +  ggtitle("Histogram of Mean Measurements") +
  theme(plot.title = element_text(hjust = 0.5)) +
    facet_wrap(~name, scales = 'free_x')
```


```{r combHistSE, warning=FALSE, error=TRUE, message=FALSE}
# create a histogram for the standard error measurements
ggplot(cancer_df[ , c(2,13:22)] %>% 
         pivot_longer(cols = radius_se:fractal_dimension_se), 
       aes(value, fill = diagnosis)) + 
    geom_histogram(bins = 10)  +  ggtitle("Histogram of Standard Error Measurements") +
  theme(plot.title = element_text(hjust = 0.5)) + 
    facet_wrap(~name, scales = 'free_x')
```

```{r combHistMaxMin, warning=FALSE, error=TRUE, message=FALSE}
# create a histogram for the maximum/worst measurements
ggplot(cancer_df[ , c(2,23:32)] %>% 
         pivot_longer(cols = radius_worst:fractal_dimension_worst), 
       aes(value, fill = diagnosis)) + 
    geom_histogram(bins = 10)  +  ggtitle("Histogram of Maximum/Worst Measurements") +
  theme(plot.title = element_text(hjust = 0.5)) + 
    facet_wrap(~name, scales = 'free_x')
```


Based on the output above, we chose to focus on the mean measurements.  The standard error and maximum/worst measurements do not add appear to add much over what is already visible in the mean measurements.


```{r hist1, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the Mean Radius of nuclei
ggplot(data = cancer_df, aes(x = radius_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Mean Radius of nuclei') 
ggplot(cancer_df, aes(x = radius_mean, color = diagnosis, fill = diagnosis)) + 
 geom_histogram(aes(y=..density..), alpha=0.5, 
                position="identity")+
 geom_density(alpha=.2)
```


```{r hist2, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the texture_mean
ggplot(data = cancer_df, aes(x = texture_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Texture Mean of nuclei') 
```

```{r hist3, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the perimeter_mean
ggplot(data = cancer_df, aes(x = perimeter_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Mean of nuclei perimeter') 
```


```{r hist4, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the area_mean
ggplot(data = cancer_df, aes(x = area_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Mean Area of nuclei') 
```

```{r hist5, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the smoothness_mean
ggplot(data = cancer_df, aes(x = smoothness_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Smoothness Mean of nuclei') 
```

```{r hist6, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the compactness_mean
ggplot(data = cancer_df, aes(x = compactness_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Compactness Mean of nuclei') 
```

```{r hist7, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the concavity_mean
ggplot(data = cancer_df, aes(x = concavity_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Concavity Mean of nuclei') 
```

```{r hist8, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the concave points_mean
ggplot(data = cancer_df, aes(x = concave_points_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Concave Points Mean of nuclei') 
```

```{r hist9, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the symmetry_mean
ggplot(data = cancer_df, aes(x = symmetry_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Symmetry Mean of nuclei') 
```

```{r hist10, warning=FALSE, error=TRUE, message=FALSE}
#Histogram of the symmetry_mean
ggplot(data = cancer_df, aes(x = fractal_dimension_mean)) + 
  geom_histogram(aes(fill = diagnosis), alpha = 0.5) +
  xlab('Fractal Dimension Mean of nuclei') 
```

```{r ggally, warning=FALSE, error=TRUE, message=FALSE}
library(GGally)
#Pair plot between variables
ggpairs(cancer_df[ , c(3:12)])
```



# 3. Split the data into test and training data.

```{r}
# First, rescale the data
# create the rescaling function we have been using thus far
rescale_x <- function(x){(x-min(x))/(max(x)-min(x))}
# create a copy of the df
rescaled_df <- cancer_df
# retain only the first two columns and all the 'mean' data
rescaled_df <- rescaled_df[1:12]
# apply the rescale function to all columns except id and diagnosis
rescaled_df[3:12] <- sapply(rescaled_df[3:12],rescale_x)
# confirm rescaling worked correctly
# all rescaled vars should be within [0,1]
summary(rescaled_df)
# Now split the data
# set the seed to Notre Dame's founding year
set.seed(1842)
# determine the number of rows in the dataframe
n <- nrow(rescaled_df)
# get a list of 20% of the rows in combined to use as indices
test_idx <- sample.int(n, size = round(0.2 * n))
# set the the training data to be those rows not matching the index list
training <- rescaled_df[-test_idx,]
# show the number of training rows
nrow(training)
glimpse(training)
# set the the test data to be those rows matching the index list
testing <- rescaled_df[test_idx,] 
# show the number of test rows
nrow(testing)
glimpse(training)
```


# 4. Build a classification algorithm using decision trees. Prune your tree appropriately.

```{r descTree, warning=FALSE, error=TRUE, message=FALSE}
# set the seed for consistent results
set.seed(1842)

# Define the formula
form <- as.formula(diagnosis ~ radius_mean + texture_mean + perimeter_mean + 
                     area_mean + smoothness_mean + compactness_mean + concavity_mean + 
                     concave_points_mean + symmetry_mean + fractal_dimension_mean)

#Generate the Decision tree
diag.tree <- rpart(form, data=training)

#Print the Tree CP
printcp(diag.tree)

#Plotting the Tree CP
plotcp(diag.tree)

#Partykit plot of the Tree
plot(as.party(diag.tree))
```

Prune the tree using CP 0.035

```{r pruneTree, warning=FALSE, error=TRUE, message=FALSE}
diag.new.tree <- prune(diag.tree, cp = 0.035)

#Partykit plot of the Pruned Tree
plot(as.party(diag.new.tree))
```

```{r fancyPlot, warning=FALSE, error=TRUE, message=FALSE}
library(rattle)
#Generate the fancy plot
fancyRpartPlot(diag.new.tree)
```

Let's use the cross-validation method from `caret` package to compare the models.

```{r caretTree, warning=FALSE, error=TRUE, message=FALSE}
# fit the model
diag.tree.caret = train(form, 
                        data = training, 
                        method = 'rpart', 
                        trControl = trainControl(method = "cv"))

#Plot the Final Tree
fancyRpartPlot(diag.tree.caret$finalModel)
```


```{r treePredict, warning=FALSE, error=TRUE, message=FALSE}
# use the decision tree created above to predict values in the test data 
# and then store the results
testing$tree_predict <- predict(diag.new.tree,newdata=testing, type="class")
# create the confusion matrix using the table function
confusion_tree <- table(testing$tree_predict, testing$diagnosis)
confusion_tree
# show the accuracy of the decision tree
cat("Overall accuracy of prediction:\t",sum(diag(confusion_tree)/nrow(testing)) 
    %>% round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying M as B:\t",(confusion_tree[1,2]/(confusion_tree[1,1]+confusion_tree[1,2])) 
    %>% round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying B as M:\t",(confusion_tree[2,1]/(confusion_tree[2,1]+confusion_tree[2,2])) 
    %>% round(4),"\n")
```


# 5. Build a classification algorithm using random forests/bagging. Adjust the parameters of the forest appropriately.

```{r randForest, warning=FALSE, error=TRUE, message=FALSE}

# set the seed for consistent results
set.seed(1842)

#Generate the Random Forest
diag.forest <- randomForest(form, mtry = 3, ntree = 500, 
                            data=training, na.action = na.roughfix)

#Print the Random Forest
diag.forest
```

```{r rfPredict, warning=FALSE, error=TRUE, message=FALSE}
# use the Random Forest created above to predict values in the test data 
# and then store the results
testing$rf_pred <- predict(diag.forest,newdata=testing, type="class")
# create the confusion matrix using the table function
confusion_rf <- table(testing$rf_pred, testing$diagnosis)
confusion_rf
# show the accuracy of the Random Forest
cat("Overall accuracy of prediction:\t",sum(diag(confusion_rf)/nrow(testing)) 
    %>% round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying M as B:\t",(confusion_rf[1,2]/(confusion_rf[1,1]+confusion_rf[1,2])) 
    %>% round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying B as M:\t",(confusion_rf[2,1]/(confusion_rf[2,1]+confusion_rf[2,2])) 
    %>% round(4),"\n")
```



```{r randForestCaret, warning=FALSE, error=TRUE, message=FALSE}

#10 folds repeat 3 times
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')
#Metric compare model is Accuracy
metric <- "Accuracy"

# set the seed for consistent results
set.seed(1842)

#Number randomely variable selected is mtry
diag.rf.caret <- train(form, 
                      data=training, 
                      method='rf', 
                      metric='Accuracy', 
                      tuneLength = 10,
                      trControl=control)

#Print the random forest cross validation results
print(diag.rf.caret)

#Plot the mtry vs Accuracy chart
plot(diag.rf.caret)
```

So we can see the highest accuracy can be achived by using mtry = 7 i.e. considering 7 predictors at a time for spliting to generate the trees in the random forest.

# 6. Build a classification algorithm using Kth Nearest Neighbors. Tune the value of K appropriately.


```{r knnModel, warning=FALSE, error=TRUE, message=FALSE}
# Choose a value for K that is equal to the square root of n,
# the numnber of onservations in the training set
k_try = sqrt(nrow(training[3:12]))
k_try
# We'll use 21 as our value of K
diag_knn_21 <- knn(training[3:12],testing[3:12],cl=training$diagnosis,k=21)
# create and display the confusion matrix
confusion_knn21 <- table(predicted = diag_knn_21, actual = testing$diagnosis)
confusion_knn21
# show the accuracy of the KNN classification
cat("Overall accuracy of prediction:\t",sum(diag(confusion_knn21)/nrow(testing)) 
    %>% round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying M as B:\t",(confusion_knn21[1,2]/(confusion_knn21[1,1]+confusion_knn21[1,2])) 
    %>% round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying B as M:\t",(confusion_knn21[2,1]/(confusion_knn21[2,1]+confusion_knn21[2,2])) 
    %>% round(4),"\n")
```


Let's tune K to see if we can get better accuracy

```{r knnCaret, warning=FALSE, error=TRUE, message=FALSE}
# set the seed for consistent results
set.seed(1842)
# set the train control to use 5-fold cross validation
# choosing 5-fold as a good middle ground
trControl <- trainControl(method  = "cv",
                          number  = 5)
#find the best knn fit using values of K of all odd numbers from 1 to 99
knn_fit <- train(diagnosis ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = c((1:50)*2 - 1)),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = training[-1])
knn_fit
```


Revising our solution to use K = 9

```{r tuneKnn, warning=FALSE, error=TRUE, message=FALSE}
# We'll use 9 as our value of K
diag_knn_9 <- knn(training[3:12],testing[3:12],cl=training$diagnosis,k=9)
# create and display the confusion matrix
confusion_knn9 <- table(predicted = diag_knn_9, actual = testing$diagnosis)
confusion_knn9
# show the accuracy of the KNN classification
cat("Overall accuracy of model:\t",sum(diag(confusion_knn9)/nrow(testing)) 
    %>% round(4),"\n")
# show the percentage of M misclassified as B
cat("Rate of misclassifying M as B:\t",(confusion_knn9[1,2]/(confusion_knn9[1,1]+confusion_knn9[1,2])) 
    %>% round(4),"\n")
# show the percentage of B misclassified as M
cat("Rate of misclassifying B as M:\t",(confusion_knn9[2,1]/(confusion_knn9[2,1]+confusion_knn9[2,2])) 
    %>% round(4),"\n")
```
